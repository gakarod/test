{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "   print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'C:/Users/vaibh/Documents/shikata/Untitled Folder/wayne.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 117290 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿They call me Mr Carter I kissed the daughter\r\n",
      "Of the deads forehead I killed the father\r\n",
      "Spilled the heart of a mildew hater\r\n",
      "I will put them body on chill like glaciers\r\n",
      "Gracias Im crazy yes its obvious\r\n",
      "Going against me is atheist\r\n",
      "I got my angels\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  \"'\" :   3,\n",
      "  'A' :   4,\n",
      "  'B' :   5,\n",
      "  'C' :   6,\n",
      "  'D' :   7,\n",
      "  'E' :   8,\n",
      "  'F' :   9,\n",
      "  'G' :  10,\n",
      "  'H' :  11,\n",
      "  'I' :  12,\n",
      "  'J' :  13,\n",
      "  'K' :  14,\n",
      "  'L' :  15,\n",
      "  'M' :  16,\n",
      "  'N' :  17,\n",
      "  'O' :  18,\n",
      "  'P' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffThey call me Mr ' ---- characters mapped to int ---- > [56 23 37 34 54  2 32 30 41 41  2 42 34  2 16 47  2]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:17]), text_as_int[:17]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "T\n",
      "h\n",
      "e\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ufeffThey call me Mr Carter I kissed the daughter\\r\\nOf the deads forehead I killed the father\\r\\nSpilled the'\n",
      "' heart of a mildew hater\\r\\nI will put them body on chill like glaciers\\r\\nGracias Im crazy yes its obvio'\n",
      "'us\\r\\nGoing against me is atheist\\r\\nI got my angels on my shoulders and a quarter of that angel dust\\r\\nI '\n",
      "'aint sniffin Im just pitchin ya honor I aint snitchin ya honor\\r\\nHate bitch niXgas bitches with power\\r'\n",
      "'\\nVacate when the kitchen get hotter\\r\\nI just sit on the counter open the cabinet close the cupboard\\r\\nP'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\ufeffThey call me Mr Carter I kissed the daughter\\r\\nOf the deads forehead I killed the father\\r\\nSpilled th'\n",
      "Target data: 'They call me Mr Carter I kissed the daughter\\r\\nOf the deads forehead I killed the father\\r\\nSpilled the'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 56 ('\\ufeff')\n",
      "  expected output: 23 ('T')\n",
      "Step    1\n",
      "  input: 23 ('T')\n",
      "  expected output: 37 ('h')\n",
      "Step    2\n",
      "  input: 37 ('h')\n",
      "  expected output: 34 ('e')\n",
      "Step    3\n",
      "  input: 34 ('e')\n",
      "  expected output: 54 ('y')\n",
      "Step    4\n",
      "  input: 54 ('y')\n",
      "  expected output: 2 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 57) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (64, None, 256)           14592     \n",
      "_________________________________________________________________\n",
      "gru_28 (GRU)                 (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (64, None, 57)            58425     \n",
      "=================================================================\n",
      "Total params: 4,011,321\n",
      "Trainable params: 4,011,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 21, 25, 29, 16, 45, 19, 42, 10, 37, 10,  9, 28, 53, 31, 55, 21,\n",
       "       36, 27, 54, 43, 27, 40,  6, 46,  7, 27, 29, 28, 48, 15, 15,  7, 37,\n",
       "       54,  7, 30, 36, 40, 12, 53, 37, 40, 44, 45, 29, 33, 17, 23, 10, 40,\n",
       "        5,  9, 16, 10, 14, 39, 10, 26, 38, 25, 46, 41, 28, 26, 26,  2, 32,\n",
       "       18, 30, 28, 14,  4, 48, 48, 24, 35, 47,  3, 27, 48, 27, 49,  4,  2,\n",
       "       42, 10, 40, 34, 37,  9, 22, 37,  6, 26, 17, 47, 14, 51,  6],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'stroke my shit break brooms\\r\\nI been high sinceniXga\\r\\nThatsdegrees Nick Lachey niXga\\r\\nHope you know t'\n",
      "\n",
      "Next Char Predictions: \n",
      " \"VRVZMpPmGhGFYxbzRgXynXkCqDXZYsLLDhyDagkIxhkopZdNTGkBFMGKjGWiVqlYWW cOaYKAssUfr'XsXtA mGkehFShCWNrKvC\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 57)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.0423827\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 18 steps\n",
      "Epoch 1/25\n",
      "18/18 [==============================] - 6s 318ms/step - loss: 3.9740\n",
      "Epoch 2/25\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 3.0616\n",
      "Epoch 3/25\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 2.6946\n",
      "Epoch 4/25\n",
      "18/18 [==============================] - 2s 134ms/step - loss: 2.4301\n",
      "Epoch 5/25\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 2.3019 2s \n",
      "Epoch 6/25\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 2.2347\n",
      "Epoch 7/25\n",
      "18/18 [==============================] - 2s 128ms/step - loss: 2.1832 1s - l\n",
      "Epoch 8/25\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 2.1348 0s - loss:\n",
      "Epoch 9/25\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 2.0851 0s - loss: 2.088\n",
      "Epoch 10/25\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 2.0366\n",
      "Epoch 11/25\n",
      "18/18 [==============================] - 2s 130ms/step - loss: 1.9896\n",
      "Epoch 12/25\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 1.9460 0s - loss: 1.9 - ETA: 0s - loss: 1.\n",
      "Epoch 13/25\n",
      "18/18 [==============================] - 2s 133ms/step - loss: 1.9009\n",
      "Epoch 14/25\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 1.8583\n",
      "Epoch 15/25\n",
      "18/18 [==============================] - 2s 128ms/step - loss: 1.8173\n",
      "Epoch 16/25\n",
      "18/18 [==============================] - 2s 137ms/step - loss: 1.7724 0s - loss: 1.771\n",
      "Epoch 17/25\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 1.7309\n",
      "Epoch 18/25\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 1.6906\n",
      "Epoch 19/25\n",
      "18/18 [==============================] - 3s 140ms/step - loss: 1.6504\n",
      "Epoch 20/25\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 1.6114\n",
      "Epoch 21/25\n",
      "18/18 [==============================] - 2s 128ms/step - loss: 1.5700\n",
      "Epoch 22/25\n",
      "18/18 [==============================] - 2s 127ms/step - loss: 1.5293\n",
      "Epoch 23/25\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 1.4932\n",
      "Epoch 24/25\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 1.4510\n",
      "Epoch 25/25\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 1.4041\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=25\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_39'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (1, None, 256)            14592     \n",
      "_________________________________________________________________\n",
      "gru_29 (GRU)                 (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (1, None, 57)             58425     \n",
      "=================================================================\n",
      "Total params: 4,011,321\n",
      "Trainable params: 4,011,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"Start \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Epoch 1 Batch 0 Loss 4.042327404022217\n",
      "Epoch 1 Loss 3.6895\n",
      "Time taken for 1 epoch 5.071139574050903 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.636118173599243\n",
      "Epoch 2 Loss 2.8645\n",
      "Time taken for 1 epoch 1.2915172576904297 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.903195381164551\n",
      "Epoch 3 Loss 2.5823\n",
      "Time taken for 1 epoch 1.2292766571044922 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.511789560317993\n",
      "Epoch 4 Loss 2.3388\n",
      "Time taken for 1 epoch 1.2732868194580078 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.395855665206909\n",
      "Epoch 5 Loss 2.2612\n",
      "Time taken for 1 epoch 1.7180371284484863 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.2354798316955566\n",
      "Epoch 6 Loss 2.2155\n",
      "Time taken for 1 epoch 1.2641708850860596 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.1939737796783447\n",
      "Epoch 7 Loss 2.1894\n",
      "Time taken for 1 epoch 1.2661690711975098 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.189877510070801\n",
      "Epoch 8 Loss 2.1072\n",
      "Time taken for 1 epoch 1.2853343486785889 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.081739664077759\n",
      "Epoch 9 Loss 2.0920\n",
      "Time taken for 1 epoch 1.276651382446289 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.038172960281372\n",
      "Epoch 10 Loss 2.0334\n",
      "Time taken for 1 epoch 1.6169886589050293 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.0211586952209473\n",
      "Epoch 11 Loss 1.9253\n",
      "Time taken for 1 epoch 1.284437656402588 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.9826867580413818\n",
      "Epoch 12 Loss 1.8979\n",
      "Time taken for 1 epoch 1.2552838325500488 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.908483624458313\n",
      "Epoch 13 Loss 1.8624\n",
      "Time taken for 1 epoch 1.2692368030548096 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.8267804384231567\n",
      "Epoch 14 Loss 1.8152\n",
      "Time taken for 1 epoch 1.2302789688110352 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.793196201324463\n",
      "Epoch 15 Loss 1.7634\n",
      "Time taken for 1 epoch 1.6302471160888672 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.7678189277648926\n",
      "Epoch 16 Loss 1.7531\n",
      "Time taken for 1 epoch 1.273284673690796 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.736933708190918\n",
      "Epoch 17 Loss 1.7254\n",
      "Time taken for 1 epoch 1.2639691829681396 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.6720389127731323\n",
      "Epoch 18 Loss 1.7167\n",
      "Time taken for 1 epoch 1.2432801723480225 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.6184051036834717\n",
      "Epoch 19 Loss 1.6186\n",
      "Time taken for 1 epoch 1.2600200176239014 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.632075548171997\n",
      "Epoch 20 Loss 1.6017\n",
      "Time taken for 1 epoch 1.6364715099334717 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.5260019302368164\n",
      "Epoch 21 Loss 1.5987\n",
      "Time taken for 1 epoch 1.300199031829834 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.5233834981918335\n",
      "Epoch 22 Loss 1.5532\n",
      "Time taken for 1 epoch 1.234436273574829 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.4996565580368042\n",
      "Epoch 23 Loss 1.4760\n",
      "Time taken for 1 epoch 1.2542834281921387 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.3662418127059937\n",
      "Epoch 24 Loss 1.4308\n",
      "Time taken for 1 epoch 1.2813048362731934 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.3738080263137817\n",
      "Epoch 25 Loss 1.4155\n",
      "Time taken for 1 epoch 1.6979339122772217 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.3319262266159058\n",
      "Epoch 26 Loss 1.3928\n",
      "Time taken for 1 epoch 1.3140432834625244 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.2667773962020874\n",
      "Epoch 27 Loss 1.2833\n",
      "Time taken for 1 epoch 1.2362761497497559 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.2527751922607422\n",
      "Epoch 28 Loss 1.2528\n",
      "Time taken for 1 epoch 1.2522871494293213 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.2190518379211426\n",
      "Epoch 29 Loss 1.2583\n",
      "Time taken for 1 epoch 1.2412803173065186 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 1.1594959497451782\n",
      "Epoch 30 Loss 1.1875\n",
      "Time taken for 1 epoch 1.6436209678649902 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 1.1394667625427246\n",
      "Epoch 31 Loss 1.1004\n",
      "Time taken for 1 epoch 1.2922930717468262 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 1.0238617658615112\n",
      "Epoch 32 Loss 1.0960\n",
      "Time taken for 1 epoch 1.2478666305541992 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 0 Loss 1.0050543546676636\n",
      "Epoch 33 Loss 1.0387\n",
      "Time taken for 1 epoch 1.2392961978912354 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.9360613822937012\n",
      "Epoch 34 Loss 0.9753\n",
      "Time taken for 1 epoch 1.2507188320159912 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.8599122762680054\n",
      "Epoch 35 Loss 0.9590\n",
      "Time taken for 1 epoch 1.7102892398834229 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.8637584447860718\n",
      "Epoch 36 Loss 0.8483\n",
      "Time taken for 1 epoch 1.3002939224243164 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.7560678124427795\n",
      "Epoch 37 Loss 0.7920\n",
      "Time taken for 1 epoch 1.2832906246185303 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.679833173751831\n",
      "Epoch 38 Loss 0.7737\n",
      "Time taken for 1 epoch 1.271287441253662 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.6289311647415161\n",
      "Epoch 39 Loss 0.7120\n",
      "Time taken for 1 epoch 1.2332775592803955 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.5969004034996033\n",
      "Epoch 40 Loss 0.6278\n",
      "Time taken for 1 epoch 1.7095251083374023 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # initializing the hidden state at the start of every epoch\n",
    "  # initally hidden is None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # saving (checkpoint) the model every 5 epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
